[{"id":0,"href":"/docs/bigdata_platform/","title":"BigData Platform","section":"Docs","content":"BigData Platform #  "},{"id":1,"href":"/docs/bigdata_platform/ApacheSpark/","title":"Apache Spark","section":"BigData Platform","content":"Apache Spark #  "},{"id":2,"href":"/docs/ETC/","title":"ETC","section":"Docs","content":"ETC #  "},{"id":3,"href":"/docs/ETC/Hugo/","title":"Hugo","section":"ETC","content":"Hugo 블로그 생성 #  "},{"id":4,"href":"/docs/ETC/Hugo/first_hugo_blog/","title":"초보자 Hugo 블로그 구축기","section":"Hugo","content":"구축 환경 #   OS : MacOS Catalina 10.15.7\n Hugo란? #  : Go 언어로 쓰여진 \u0026lsquo;정적 웹사이트 생성기\u0026rsquo;(Static site generator) 라고 한다.\nGo 언어란? : 구글에서 발표한 언어로, 파이썬 같은 스크립트 언어가 아닌 컴파일 언어이다.\n정적 웹사이트란? (Static site) :\n 말 그대로 웹사이트가 \u0026lsquo;정적\u0026rsquo;(Static)인 사이트이다. 우리가 봐오는 일반적인 블로그처럼 사용자가 접속할 때마다 고정된 화면을 보여준다. 정적이지 않은(Dynamic) 웹사이트의 대표적인 예로는 유튜브, 네이버 등 이 있다. 이런 사이트들은 내가 접속을 할 때마다 사이트의 화면이 바뀔 수 있다. 정적인 웹사이트의 구축은 그렇지 않은 사이트보다 더 구축하기 쉽고 빠르다는 장점이 있다.  →  따라서 Hugo는 이런 정적 사이트를 만들어 줄 수 있는 생성기라고만 이해하고 넘어갔다.\n그렇다면 왜 Hugo 를 선택했는가?\n 사실 큰 이유는 없다 ㅎㅎ 그냥 일반적으로 많이 쓰이는 것으로 보이는 Jekyll보다 사이트 생성이 빠르다고 하였고 Go언어를 공부해보고자 하는 사람들이 많이 시도해 본 것으로 보인다. 어차피 제로 베이스로 시작하는거, 나에겐 Jekyll이나 Hugo나 외계어로 보이는건 똑같았다.  자 이제 생성을 시작해보자!\nSTEP1. Hugo 설치 #  나는 맥북이므로 brew를 활용하여 설치하였다.\nbrew 가 없다면 brew 에서 설치 해 준다.\n$ brew install hugo ## 설치 후 버전 확인으로 설치가 잘 됏는지 확인해보자 $ hugo version 윈도우이거나 brew 이외의 설치 방법은 Hugo 공식 사이트인 https://gohugo.io/getting-started/quick-start/ 나 하단 Reference의 블로그들을 참고해 주시길 바란다.\nSTEP2. Hugo 블로그 폴더 생성 #  사실 용어는 잘 모르겠다.ㅎㅎ\n그냥 쉽게 생각하면 앞으로 블로그에 올라갈 콘텐츠들이나 테마 등 모든 것이 담기는 폴더라고 생각하면 된다.\n## 폴더 이름은 마음대로 지정해도 된다.  ## 나는 blog라고 지었는데 변경을 원하면 blog 대신 다른 이름을 쓰면 된다. $ hugo new site blog 나는 그냥 /Users/내이름/ 경로에서 명령어를 입력하여 생성했다.\n생성 되면 Hugo site가 생성됐다며 아래와 같은 메세지가 출력된다!\nCongratulations! Your new Hugo site is created in /Users/내이름/blog. Just a few more steps and you\u0026#39;re ready to go: ** 1. Download a theme into the same-named folder. Choose a theme from https://themes.gohugo.io/ or create your own with the \u0026#34;hugo new theme \u0026lt;THEMENAME\u0026gt;\u0026#34; command. 2. Perhaps you want to add some content. You can add single files with \u0026#34;hugo new \u0026lt;SECTIONNAME\u0026gt;/\u0026lt;FILENAME\u0026gt;.\u0026lt;FORMAT\u0026gt;\u0026#34;. 3. Start the built-in live server via \u0026#34;hugo server\u0026#34;. Visit https://gohugo.io/ for quickstart guide and full documentation. /Users/내이름/blog 폴더에 들어가면 아래와 같은 기본적인 구조가 생성 돼 있다.\nSTEP3. 테마 추가하기 #  나는 다른 분들이 이쁘게 만들어주신 테마를 사용할 것이다.ㅎㅎ\nhugo theme 사이트 : themes.gohugo.io\n내가 선택한 테마 : https://themes.gohugo.io/hugo-book/\n Hugo 버전이 최신 버전 이어야하면 upgrade 해준다.  ## 버전 확인 ; 0.78.0 이상이 아니라면 upgrade 필요하다. $ hugo version ## (참고) 나는 brew로 간단하게 upgrade 해줬다. $ brew upgrade hugo 이제 내가 고른 테마를 다운 받자!\n\u0026ndash;\u0026gt; 저 테마의 github : hhttps://github.com/alex-shpak/hugo-book.git\n## 우선, STEP2에서 만든 blog 경로로 들어가자 $ cd blog ## 테마 다운로드 $ git submodule add https://github.com/alex-shpak/hugo-book.git themes/book 이제 아까 blog 폴더 내에 themes에 들어가보면 \u0026lsquo;book\u0026rsquo; 폴더가 생긴 것을 볼 수 있다.\n그 다음, 보통 글을 한개를 써서 올려보라는데 사실 나는 어떻게 해야하는지 감이 없어서 애를 먹었더랬다.\n뒤에 할 터이니 우선 다음 step으로 넘어가자!\nSTEP4. Github 저장소 생성 #  https://gohugo.io/hosting-and-deployment/hosting-on-github/\nGithub 저장소 2개를 생성 해야 한다.\n 하나는 블로그 컨텐츠 저장소 생성 (blog) 다른 하나는 렌더링된 웹사이트를 담을 저장소 (key4920.github.io)  사실 뭔소린지 잘 모른다. 내가 아는 것은 2개를 생성해야하고, 2번인 웹사이트를 담을 저장소는 이름을 github.io 로 지어야 한다는 것이다.\n\u0026lt; 나의 첫번째 뻘짓 \u0026gt;\n2번 저장소의 USERNAME 부분이 내 id 이어야 한다는 것을 무시하고, 내 마음대로 지어보려 했다가 자꾸 에러가 났었다.\n반드시 본인의 github id로 생성해 주어야한다. 바꿀 수 있는 것 같지만, 나중에 시간이 남으면 시도해 보도록하고, 패스한다.\n두개 모두 이미 가지고 있으므로 already exists라고 뜨지만 그냥 저렇게 생성해주면 된다.\n1번 저장소인 \u0026lsquo;blog\u0026rsquo;를 내가 처음에 만들었던 \u0026lsquo;blog\u0026rsquo; 폴더랑 연동 해 주고\n2번 저장소인 \u0026lsquo;key4920.github.io\u0026rsquo;를 submodule로 등록 해 준다.\n## 우선 처음 만들었던 blog 폴더로 이동 (이미 들어와 있으면 패스) cd blog ## git 연동 $ git init $ git remote add origin https://github.com/key4920/blog.git $ git submodule add -b master git@github.com:key4920/key4920.github.io.git public \u0026lt;나의 두번째 뻘짓 \u0026gt;\n blog remote할 때 Permission denied (publickey).에러가 났다.   잘 모르지만 github 주소를 \u0026lsquo;https\u0026rsquo; \u0026lsquo;ssh\u0026rsquo; 중에 \u0026lsquo;https\u0026rsquo; 주소로 하니 됐고  key4920.github.io submodule 할 때는 Unable to checkout submodule ‘public’ 에러가 났다.   얘는 \u0026lsquo;ssh\u0026rsquo; 주소로 하니 됐다\u0026hellip; 왜인지는 모르겠다\u0026hellip;  STEP5. 예시 사이트 보기 #  아까 언급했듯이, 막상 글을 쓰려니 어떻게 해야하는지 막막했다.\n그런데 보통 themes/book 폴더 내에 들어가보면 \u0026lsquo;exampleSite\u0026rsquo; 폴더가 있다.\n\u0026lsquo;exampleSite\u0026rsquo; 폴더 안에 있는 모든 폴더 및 파일들을 복사해서 blog 폴더 안에 덮어쓰기 해준다.\n 이때 내가 고른 테마의 exampleSite 안에는 \u0026lsquo;config.yaml\u0026rsquo; 파일이 있는데\n이건 기본적으로 생성 돼 있던 \u0026lsquo;config.toml\u0026rsquo; 파일과 동일한 것이지만 파일 확장자가 달라서 \u0026lsquo;덮어쓰기\u0026rsquo;가 안됐으므로 \u0026lsquo;config.toml\u0026rsquo; 파일은 삭제 해 줬다.  $ hugo server -D 입력해주면 아래와 같은 문구와 함께 http://localhost:1313/ 에 접속하라고 뜨는데, 이 url은 내 사이트를 build하기 전 예시로 보여주는 기능이다.\n웹에 이 url을 복붙해서 들어가면 현재 내 blog의 예시 사이트를 볼 수 있다.\n나 같은 초보자들은 이제 exampleSite에서 복붙했던 content 내의 예시 글들을 참고해서 blog 내의 구조를 익히면 된다.\n내가 고른 테마 같은 경우엔, 블로그에 올라가는 일반적인 글 경로는 blog/content/post/콘텐츠이름 에 있었다.\n\u0026lt; 나의 세번째 뻘짓 \u0026gt;\n이건 내가 써봤던 \u0026lsquo;stack\u0026rsquo; 테마에 한정된 이야기 이지만, \u0026lsquo;config.yaml\u0026rsquo; 안에 dateFormat 이 있었는데 모두 #를 앞에 붙여서 주석처리 해 주었다. 이게 있으니 자꾸 콘텐츠의 글의 날짜가 이상한 형식으로 보였다.\n더이상 지원하지 않는다고하니 주석처리 해주거나 지워준다.\n여태까지는 blog 폴더 안에 어느 형식으로 어떤 구조로 콘텐츠를 넣으면 어떻게 보이는지를 살펴보았다. 하지만, 실제 내 블로그 url인 key4920.github.io 를 입력해서 들어가도 사이트가 build 된 상태가 아니다.\n앞의 http://localhost:1313/ 에서 결과가 맘에 들면 git 업로드 해줘야 실제 내 블로그에 적용된다.\nSTEP6. 사이트에 글 업로드 #  blog 폴더에 들어가서 \u0026lsquo;deploy.sh\u0026rsquo; 파일을 하나 생성하고 파일 안에 아래 내용을 복붙해서 저장해준다.\n 이때 hugo -t book 부분에서 \u0026lsquo;book\u0026rsquo;을 본인 테마 이름으로 수정 해 준다.  #!/bin/sh # If a command fails then the deploy stops set -e printf \u0026#34;\\033[0;32mDeploying updates to GitHub...\\033[0m\\n\u0026#34; # Build the project. ####### 이부분을 본인 테마로 변경해주세요~ ####### hugo -t book # Go To Public folder cd public # Add changes to git. git add . # Commit changes. msg=\u0026#34;rebuilding site $(date)\u0026#34; if [ -n \u0026#34;$*\u0026#34; ]; then msg=\u0026#34;$*\u0026#34; fi git commit -m \u0026#34;$msg\u0026#34; # Push source and build repos. git push origin master cd .. # anyblogname 업데이트 git add . msg=\u0026#34;rebuilding site `date`\u0026#34; if [ $# -eq 1 ] then msg=\u0026#34;$1\u0026#34; fi git commit -m \u0026#34;$msg\u0026#34; git push origin master 원래는 이 파일 안에 한줄 한줄을 터미널에서 매번 실행 해 주어야하지만, 귀찮으므로 이런 파일을 만들고 파일만 실행하면 안의 코드가 실행되게 하는 원리이다.\n이제 이 파일을 실행해보자!\n## 뒤에는 그냥 커밋 메시지이다.  $ ./deploy.sh \u0026#34;message\u0026#34; \u0026lt; 나의 네번째 뻘짓 \u0026gt;\n이때 나는 권한이 없다며 에러가 났다. 아래 코드로 권한 지정해주자\n또한 가끔 내가 변경한 파일이 untracked file이어서 Github로 push가 안 된다. 이럴 때는 위의 deploy.sh 파일 내의 코드들을 한줄씩 직접 실행 해 준다.\n## 권한 에러 있었다면 지정 chmod +x deploy.sh 그래도 혹시 권한 에러가 난다면, System Preferences \u0026gt; Security \u0026amp; Privacy \u0026gt; Privacy \u0026gt; Full Disk Access 에 Terminal 을 추가해준다. 나같은 경우는 그래도 권한 에러가 났다. 그래서 아래 코드로 해결했다.\n$ xattr -l deploy.sh ## 프린트 된 것 com.apple.quarantine 이 있다면 아래 명령어로 삭제 $ xattr -d com.apple.quarantine deploy.sh ## 삭제 ## 다시 권한 부여 후 파일 실행 $ chmod +x deploy.sh $ ./deploy.sh \u0026#34;message\u0026#34;  앞으로 블로그 업데이트에 필요한 일 #   content 폴더 내에 마크다운 형식으로 글 파일 넣거나 (자세한 경로는 본인 테마마다 상이할 수 있습니다.) config 등 다른 파일 customising 콘텐츠 업로드에 필요한 코드  ## 본인 hugo blog 폴더 들어가기 $ cd blog ## (옵션) 예시 사이트를 보는 것이므로 옵션  $ hugo server -D ## 빠져나오려면 Ctrl + C $ ./deploy.sh \u0026#34;commit message\u0026#34;  추가 기능 #   config 파일에서 원하는대로 이것저것 변경 해준다.  DefaultContentLanguage: ko 로 언어도 수정해주고 favicon이나 이미지도 넣어주는 등 다양하게 커스터마이징 해 주었다!   구글이나 네이버, 다음에 글이 검색 될 수 있도록 노출 시키는 기능 추가 또는 구글 애드센스 넣기 댓글 기능 넣기  등 다양하게 기능을 넣어 본인의 블로그를 구축해 나갈 수 있다~\n앞으로 다른 글에서 다뤄보도록 하겠다.\n Reference #  https://gohugo.io/getting-started/quick-start/\nhttps://gohugo.io/hosting-and-deployment/hosting-on-github/\nhttps://github.com/Integerous/Integerous.github.io\nhttps://hoontaeklee.github.io/20191229_blogging_with_hugo/#2-unable-to-checkout-submodule-public\n"},{"id":5,"href":"/docs/bigdata_platform/Hadoop/","title":"Hadoop","section":"BigData Platform","content":"Hadoop #  "},{"id":6,"href":"/docs/bigdata_platform/Hadoop/hadoop_install/","title":"Mac OS에 하둡(Hadoop) 설치","section":"Hadoop","content":"설치 환경 #   OS : MacOS Catalina 10.15.7\nHadoop : 3.3.0\n STEP1. 하둡 설치 #  터미널에 명령 입력\nbrew install hadoop 만약 brew가 없다면 Homebrew에 접속하여 설치해준다.\nSTEP2. 환경변수 수정 #  ## 하둡 버전 X.X.X는 본인이 설치한 버전으로 수정  cd /usr/local/cellar/hadoop/3.3.0/libexec/etc/hadoop Finder에서 저 경로가 보이지 않았지만, 터미널에서 명령어 치니 경로가 잘 들어가지긴 했다.\n그래도 직접 Finder에서 보기 위해서 Finder에서 \u0026lsquo;Cmd+Shift+G\u0026rsquo; 를 치니 경로 검색 창이 떴고 그걸로 들어가니 Finder에서 볼 수 있었다.\n수정해야 할 파일들\n (1) hadoop-env.sh\n(2) core-site.xml\n(3) hdfs-site.xml\n(4) mapred-site.xml\n(5) yarn-site.xml\n 2-1. hadoop-env.sh 변경 #  open hadoop-env.sh 파일을 열어서 HADOOP_OPTS 부분을 변경해 줘야한다.\n열어보니 아래 주석처리된 부분만 적혀있어서 마지막 한줄을 추가해서 입력해 주었다.\nexport HADOOP_OPTS=”-Djava.net.preferIPv4Stack=true -Djava.security.krb5.realm= -Djava.security.krb5.kdc=” 또한, 터미널에 아래 명령어 입력하면 java경로를 보여준다.\n/usr/libexec/java_home 이때 나온 경로 또한 \u0026lsquo;hadoop-env.sh\u0026rsquo; 파일 안에 없다면 추가해준다.\nexport JAVA_HOME=“/Library/Java/JavaVirtualMachines/adoptopenjdk-8.jdk/Contents/Home” 2-2. core-site.xml 변경 #  open core-site.xml 파일을 열어  태그에 복사하여 넣어준다.\n\u0026lt;configuration\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hadoop.tmp.dir\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;/usr/local/Cellar/hadoop/hdfs/tmp\u0026lt;/value\u0026gt; \u0026lt;description\u0026gt;A base for other temporary directories.\u0026lt;/description\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;fs.default.name\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;hdfs://localhost:9000\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;/configuration\u0026gt; 2-3. hdfs-site.xml 변경 #  open hdfs-site.xml 파일을 열어  태그에 복사하여 넣어준다.\n\u0026lt;configuration\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;dfs.replication\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;1\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;/configuration\u0026gt; 2-4. mapred-site.xml 변경 #  open mapred-site.xml 파일을 열어  태그에 복사하여 넣어준다.\n\u0026lt;configuration\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;mapreduce.framework.name\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;yarn\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;mapreduce.application.classpath\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;/configuration\u0026gt; 2-5. yarn-site.xml 변경 #  open yarn-site.xml 파일을 열어  태그에 복사하여 넣어준다.\n\u0026lt;configuration\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;yarn.nodemanager.aux-services\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;mapreduce_shuffle\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;yarn.nodemanager.env-whitelist\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;/configuration\u0026gt; STEP3. 하둡 실행 #  3-1. 실행 전 준비 #  ssh localhost 터미널에 위 명령어를 입력해 봤을 때 마지막 접속시간이 안뜨고 Connection refused 라고 뜨면,\n아래 명령어를 입력해준다.\nssh-keygen -t rsa -P \u0026#39;\u0026#39; -f ~/.ssh/id_rsa cat ~/.ssh/id_rsa.pub \u0026gt;\u0026gt; ~/.ssh/authorized_keys chmod 0600 ~/.ssh/authorized_keys 그냥 hadoop 폴더 내에서 입력했더니 ./ssh 폴더가 없다고 떴다.\n나 같은 경우는 가장 하위 홈 디렉토리에서 명령어 입력하니 됐다. (/Users/\u0026ldquo;user_name\u0026rdquo;/)\n이제 HDFS 로 포맷한다.\ncd /usr/local/cellar/hadoop/3.3.0/libexec/bin hdfs namenode -format 3-2. 하둡 실행 #  cd /usr/local/cellar/hadoop/3.3.0/libexec/sbin ./start-all.sh # 또는 ./start-dfs.sh # 또는 ./start-yarn.sh 위 명령어를 입력해주면 정상적으로 실행된다.\n만약 아래 같은 에러가 나면 원격 로그인을 허용하지 않은 것이므로 환경설정에서 허용 해준다.\n\u0026#34;localhost: ssh: connect to host localhost port 22: Connection refused\u0026#34; 환경설정 \u0026gt; 공유 \u0026gt; 원격 로그인에 체크박스를 눌러준다.\n3-3. 실행 확인 #  jps jps 를 터미널에 입력해주면, 하둡이 정상 설치 및 실행되고 있음을 아래와 같이 보여준다.\n29171 NodeManager 28644 NameNode 29255 Jps 28745 DataNode 28206 ResourceManager 28879 SecondaryNameNode 그럼 이제 localhost 로 접속해서 확인해보자\n Cluster status : http://localhost:8088\nHDFS status : http://localhost:9870 Secondary NameNode status : http://localhost:9868\n Cluster status #  HDFS status #  3-4. 실행 종료 #  ## 만약 경로가 아래 경로에 들어와 있지 않다면 다시 들어가준다.  ## 하지만 해당 경로에서 ./start-all.sh 로 실행 해 줬기 때문에  ## 그냥 아래 ./stop-all.sh만 실행해주면 된다. # cd /usr/local/cellar/hadoop/3.3.0/libexec/sbin ./stop-all.sh # 또는 ./stop-dfs.sh # 또는 ./stop-yarn.sh Reference #  Installing Hadoop on a Mac\nmacOS에서 Hadoop 설치하기\n"},{"id":7,"href":"/docs/bigdata_platform/ApacheSpark/spark_install/","title":"Mac OS에 스파크3 설치 및 pyspark 시작","section":"Apache Spark","content":"설치 환경\n OS : MacOS Catalina 10.15.7\nJava : java 11.0.1\nPython : Python 3.7.6\nSpark : spark 3.0.1\n STEP1. 스파크 버전 선택 후 다운로드 #  NT. Java 와 Python은 설치 돼 있다는 가정으로 진행한다.\n스파크 다운로드 사이트 링크 : Downloads | Apache Spark\n사이트에 접속하면 아래 화면이 나오게 된다.\n원래는 Spark 2.4.7 을 선택해서 설치했었다.\n하지만 로컬의 Java 버전이 11이었으므로, 그냥 이참에 Spark3.0을 시도해보기로 했다!\n(참고) Spark3.0은 Java 11을 지원하고, Spark2.X는 Java 8을 지원한다.\n우선 원하는 Spark 버전 선택하여 \u0026lsquo;Download Spark\u0026rsquo; 를 클릭해주면 다음과 같은 화면으로 넘어간다.\nmirror sites 중 한개를 선택해 다운로드 해주는데, 그냥 최상단 링크를 눌러 다운로드 해준다.\nSTEP2. 다운로드한 압축 파일을 home directory에 풀어준다. #  tar -zxvf spark-3.0.1-bin-hadoop3.2.tar STEP3. ~/.bash_profile 환경변수 수정 #  어떤 directory에서도 spark notebook 열 수 있도록 환경변수 설정해준다.\n## .bash_profile 있는지 확인 ls -a ## .bash_profile 파일 열기 nano .bash_profile ## 또는  vi .bash_profile 아래 환경변수 추가로 써준다.\n## 버전 맞게 써주기 export SPARK_PATH=~/spark-3.0.1-bin-hadoop3.2 ## PySpark Shell을 주피터 노트북에서 열 수 있도록 설정 export PYSPARK_DRIVER_PYTHON=\u0026#34;jupyter\u0026#34; export PYSPARK_DRIVER_PYTHON_OPTS=\u0026#34;notebook\u0026#34; ## 파이썬 3를 사용한다면 아래 설정 추가해 주어야 에러가 나지 않음 export PYSPARK_PYTHON=python3 ## sparknb는 명령어 마음대로 써준 것이므로, 원하는 이름으로 수정 가능 ## local[2] 는 로컬 코어 2개를 사용한다는 뜻으로 본인 로컬 환경에 맞게 수정 가능 alias sparknb=\u0026#39;$SPARK_PATH/bin/pyspark --master local[2]\u0026#39; 만약 nano .bash_profile 했었으면 ^x —\u0026gt; Y —\u0026gt; enter 로 저장하고 빠져나오기\nvi .bash_profile 했었으면 :wq 입력 후 저장하고 빠져나오기\nsource .bash_profile STEP4. 실행 확인 #  spark-shell 실행 #  ## 터미널에서 spark-shell 입력하여 스파크 쉘 실행 spark-shell 터미널에 spark-shell 입력하면 아래와 같이 정상적으로 실행되면서 스칼라를 통해 코드를 작성할 수 있다.\n주피터 노트북에서 실행 #  ## 터미널에서 sparknb입력하여 주피터 노트북 실행 ## 위에서 alias 다른 이름으로 설정했다면, 해당 이름으로 실행 sparknb 명령어 입력하면 주피터 노트북이 켜지고, pyspark를 작성할 수 있다.\nSTEP 5. 이제 모두 완료됐으니, 간단한 실습해보자! #  주피터 노트북에서 파이썬 코드 작성 #  # Reason why we have the getOrCreate code # http://stackoverflow.com/questions/28999332/how-to-access-sparkcontext-in-pyspark-script sc = SparkContext.getOrCreate() import numpy as np TOTAL = 1000000 dots = sc.parallelize([2.0 * np.random.random(2) - 1.0 for i in range(TOTAL)]).cache() print(\u0026#34;Number of random points:\u0026#34;, dots.count()) stats = dots.stats() print(\u0026#39;Mean:\u0026#39;, stats.mean()) print(\u0026#39;stdev:\u0026#39;, stats.stdev()) Number of random points: 1000000 Mean: [-0.00076914 0.00090079]\nstdev: [0.57721218 0.57750566]\ndots.collect()[:5] ## 형식 구경하기 [array([0.71638614, 0.34966844]),\narray([0.85926697, 0.43265725]),\narray([-0.12204166, -0.8161978 ]),\narray([-0.96733469, -0.57462963]),\narray([-0.13704113, -0.48361848])]\ndots ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:262\n%matplotlib inline from operator import itemgetter from matplotlib import pyplot as plt plt.figure(figsize = (10, 5)) # Plot 1 plt.subplot(1, 2, 1) plt.xlim((-1.0, 1.0)) plt.ylim((-1.0, 1.0)) sample = dots.sample(False, 0.01) X = sample.map(itemgetter(0)).collect() Y = sample.map(itemgetter(1)).collect() plt.scatter(X, Y) # Plot 2 plt.subplot(1, 2, 2) plt.xlim((-1.0, 1.0)) plt.ylim((-1.0, 1.0)) inCircle = lambda v: np.linalg.norm(v) \u0026lt;= 1.0 dotsIn = sample.filter(inCircle).cache() dotsOut = sample.filter(lambda v: not inCircle(v)).cache() # inside circle Xin = dotsIn.map(itemgetter(0)).collect() Yin = dotsIn.map(itemgetter(1)).collect() plt.scatter(Xin, Yin, color = \u0026#39;r\u0026#39;) # outside circle Xout = dotsOut.map(itemgetter(0)).collect() Yout = dotsOut.map(itemgetter(1)).collect() plt.scatter(Xout, Yout) \u0026lt;matplotlib.collections.PathCollection at 0x7f88920d0a50\u0026gt;\nReference #  Install Spark on Mac (PySpark)\n[mac] Apache Spark Study -1 ( Spark설치(HomeBrew) )\napache-spark@3.0.1 설치\n"}]